{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7792e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "gpu=int(input(\"Which gpu number you would like to allocate:\"))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(gpu)\n",
    "model_name=int(input(\"Which model you would like to train(TYPE THE NUMBER ONLY LIKE 1)? 1. MOBILE VIT\"))\n",
    "import glob\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import re\n",
    "import datetime\n",
    "import keras\n",
    "from tensorflow.keras.layers import  Input,Conv2D,BatchNormalization,Activation,Subtract,LeakyReLU,Add,Average,Lambda,MaxPool2D,Dropout,UpSampling2D,Concatenate,Multiply,GlobalAveragePooling2D,Dense,ZeroPadding2D,AveragePooling2D\n",
    "from tensorflow.keras.layers import concatenate,Flatten,Layer,ReLU, MaxPooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skimage.feature import hog,local_binary_pattern\n",
    "from skimage import data, exposure\n",
    "from skimage.transform import radon, rescale\n",
    "from skimage.filters import roberts, sobel, scharr, prewitt\n",
    "from classification_models.keras import Classifiers\n",
    "from skimage import feature\n",
    "import os,glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import argparse\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "# Display Image\n",
    "from PIL import Image\n",
    "# computer vision package to read dataset\n",
    "import cv2\n",
    "import os\n",
    "import datetime\n",
    "from tensorflow.keras.layers import  Input,Conv2D,BatchNormalization,Activation,Subtract,LeakyReLU,Add,Average,Lambda,MaxPool2D,Dropout,UpSampling2D,Concatenate,Multiply,GlobalAveragePooling2D,Dense,ZeroPadding2D,AveragePooling2D\n",
    "from tensorflow.keras.layers import concatenate,Flatten,ConvLSTM2D,LayerNormalization,GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import loadtxt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skimage.feature import hog,local_binary_pattern\n",
    "from skimage import data, exposure\n",
    "from tensorflow.keras.layers import Layer\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as L\n",
    "\n",
    "\n",
    "def inverted_residual_block(inputs, num_filters, strides=1, expansion_ratio=1):\n",
    "    ## Point-Wise Convolution\n",
    "    x = L.Conv2D(\n",
    "        filters=expansion_ratio*inputs.shape[-1],\n",
    "        kernel_size=1,\n",
    "        padding=\"same\",\n",
    "        use_bias=False\n",
    "    )(inputs)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Activation(\"swish\")(x)\n",
    "\n",
    "    ## Depth-Wise Convolution\n",
    "    x = L.DepthwiseConv2D(\n",
    "        kernel_size=3,\n",
    "        strides=strides,\n",
    "        padding=\"same\",\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Activation(\"swish\")(x)\n",
    "\n",
    "    ## Point-Wise Convolution\n",
    "    x = L.Conv2D(\n",
    "        filters=num_filters,\n",
    "        kernel_size=1,\n",
    "        padding=\"same\",\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "\n",
    "    ## Residual Connection\n",
    "    if strides == 1 and (inputs.shape == x.shape):\n",
    "        return L.Add()([inputs, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def mlp(x, mlp_dim, dim, dropout_rate=0.1):\n",
    "    x = L.Dense(mlp_dim, activation=\"swish\")(x)\n",
    "    x = L.Dropout(dropout_rate)(x)\n",
    "    x = L.Dense(dim)(x)\n",
    "    x = L.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def transformer_encoder(x, num_heads, dim, mlp_dim):\n",
    "    skip_1 = x\n",
    "    x = L.LayerNormalization()(x)\n",
    "    x = L.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=dim\n",
    "    )(x, x)\n",
    "    x = L.Add()([x, skip_1])\n",
    "\n",
    "    skip_2 = x\n",
    "    x = L.LayerNormalization()(x)\n",
    "    x = mlp(x, mlp_dim, dim)\n",
    "    x = L.Add()([x, skip_2])\n",
    "\n",
    "    return x\n",
    "\n",
    "def mobile_vit_block(inputs, num_filters, dim, patch_size=2, num_layers=1):\n",
    "    B, H, W, C = inputs.shape\n",
    "\n",
    "    ## 3x3 conv\n",
    "    x = L.Conv2D(\n",
    "        filters=C,\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        use_bias=False\n",
    "    )(inputs)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Activation(\"swish\")(x)\n",
    "\n",
    "    ## 1x1 conv: d-dimension\n",
    "    x = L.Conv2D(\n",
    "        filters=dim,\n",
    "        kernel_size=1,\n",
    "        padding=\"same\",\n",
    "        use_bias=False\n",
    "    )(inputs)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Activation(\"swish\")(x)\n",
    "\n",
    "    ## Reshape x to flattened patches\n",
    "    P = patch_size*patch_size\n",
    "    N = int(H*W//P)\n",
    "    x = L.Reshape((P, N, dim))(x)\n",
    "\n",
    "    ## Transformr Encoder\n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_encoder(x, 1, dim, dim*2)\n",
    "\n",
    "    ## Reshape\n",
    "    x = L.Reshape((H, W, dim))(x)\n",
    "\n",
    "    ## 1x1 conv: C-dimension\n",
    "    x = L.Conv2D(\n",
    "        filters=C,\n",
    "        kernel_size=1,\n",
    "        padding=\"same\",\n",
    "        use_bias=False\n",
    "    )(inputs)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Activation(\"swish\")(x)\n",
    "\n",
    "    ## Concatenation\n",
    "    x = L.Concatenate()([x, inputs])\n",
    "\n",
    "    ## 3x3 conv\n",
    "    x = L.Conv2D(\n",
    "        filters=num_filters,\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        use_bias=False\n",
    "    )(inputs)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Activation(\"swish\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def MobileViT(input_shape, num_channels, dim, expansion_ratio, num_layers=[2, 4, 3], num_classes=3):\n",
    "    ## Input layer\n",
    "    inputs = L.Input(input_shape)\n",
    "\n",
    "    ## Stem\n",
    "    x = L.Conv2D(\n",
    "        filters=num_channels[0],\n",
    "        kernel_size=3,\n",
    "        strides=2,\n",
    "        padding=\"same\",\n",
    "        use_bias=False\n",
    "    )(inputs)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Activation(\"swish\")(x)\n",
    "    x = inverted_residual_block(x, num_channels[1], strides=1, expansion_ratio=expansion_ratio)\n",
    "\n",
    "    ## Stage 1\n",
    "    x = inverted_residual_block(x, num_channels[2], strides=2, expansion_ratio=expansion_ratio)\n",
    "    x = inverted_residual_block(x, num_channels[3], strides=1, expansion_ratio=expansion_ratio)\n",
    "    x = inverted_residual_block(x, num_channels[4], strides=1, expansion_ratio=expansion_ratio)\n",
    "\n",
    "    ## Stage 2\n",
    "    x = inverted_residual_block(x, num_channels[5], strides=2, expansion_ratio=expansion_ratio)\n",
    "    x = mobile_vit_block(x, num_channels[6], dim[0], num_layers=num_layers[0])\n",
    "\n",
    "    ## Stage 3\n",
    "    x = inverted_residual_block(x, num_channels[7], strides=2, expansion_ratio=expansion_ratio)\n",
    "    x = mobile_vit_block(x, num_channels[8], dim[1], num_layers=num_layers[1])\n",
    "\n",
    "    ## Stage 4\n",
    "    x = inverted_residual_block(x, num_channels[9], strides=2, expansion_ratio=expansion_ratio)\n",
    "    x = mobile_vit_block(x, num_channels[10], dim[2], num_layers=num_layers[2])\n",
    "    x = L.Conv2D(\n",
    "        filters=num_channels[11],\n",
    "        kernel_size=1,\n",
    "        padding=\"same\",\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Activation(\"swish\")(x)\n",
    "\n",
    "    ## Classifier\n",
    "    x = L.GlobalAveragePooling2D()(x)\n",
    "    outputs = L.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def MobileViT_S(input_shape, num_classes):\n",
    "    num_channels = [16, 32, 64, 64, 64, 96, 144, 128, 192, 160, 240, 640]\n",
    "    dim = [144, 192, 240]\n",
    "    expansion_ratio = 4\n",
    "\n",
    "    return MobileViT(\n",
    "        input_shape,\n",
    "        num_channels,\n",
    "        dim,\n",
    "        expansion_ratio,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "\n",
    "   \n",
    "def data_augmentation(normal_files,covid_files,pneumonia_files):\n",
    "    aug_normal=[]\n",
    "    aug_covid=[]\n",
    "    thresh_hold=7\n",
    "    aug_pneumonia=[]\n",
    "    \n",
    "    #x = tf.keras.preprocessing.image.load_img(\"/content/IM-0001-0001.jpeg\")\n",
    "    \n",
    "    datagen=ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "\n",
    "    )\n",
    "    #normal\n",
    "    counter=0\n",
    "    print(\"normal files:\",len(normal_files))\n",
    "    print(\"covid files:\",len(covid_files))\n",
    "    print(\"pneumonia files:\",len(pneumonia_files))\n",
    "    for location in tqdm(normal_files):\n",
    "        counter=0\n",
    "\n",
    "        x = Image.open(location).convert('L')\n",
    "        x = asarray(x)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        x = clahe.apply(x)\n",
    "        #x=img_to_array(x)\n",
    "        x=cv2.resize(x,(256,256),interpolation = cv2.INTER_CUBIC)\n",
    "        x=x/255.0\n",
    "        x = np.expand_dims(x, axis=-1) \n",
    "        x=x.reshape((1,)+x.shape)\n",
    "        #x=x/255.0\n",
    "\n",
    "\n",
    "        for i in datagen.flow(x):\n",
    "            if counter>=5:\n",
    "                break\n",
    "            #i=i/255.0\n",
    "\n",
    "            #i = cv2.resize(i,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "            aug_normal.append(i)\n",
    "            counter+=1\n",
    "    #covid\n",
    "    counter=0\n",
    "    for location in tqdm(covid_files):\n",
    "        counter=0\n",
    "        x = Image.open(location).convert('L')\n",
    "        x = asarray(x)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        x = clahe.apply(x)\n",
    "#         x = Image.open(location).convert('L')\n",
    "#         x = asarray(x)\n",
    "        x=cv2.resize(x,(256,256),interpolation = cv2.INTER_CUBIC)\n",
    "        x=x/255.0\n",
    "        x = np.expand_dims(x, axis=-1) \n",
    "        #x=img_to_array(x)\n",
    "        x=x.reshape((1,)+x.shape)\n",
    "        #x=x/255.0\n",
    "\n",
    "\n",
    "        for i in datagen.flow(x):\n",
    "            if counter>=1:\n",
    "                break\n",
    "\n",
    "            aug_covid.append(i)\n",
    "            counter+=1    \n",
    "    #pneumonia\n",
    "    counter=0\n",
    "    for location in tqdm(pneumonia_files):\n",
    "        counter=0\n",
    "        x = Image.open(location).convert('L')\n",
    "        x = asarray(x)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        x = clahe.apply(x)\n",
    "        x=cv2.resize(x,(256,256),interpolation = cv2.INTER_CUBIC)\n",
    "        x=x/255.0\n",
    "        x = np.expand_dims(x, axis=-1) \n",
    "        \n",
    "        #x=img_to_array(x)\n",
    "        x=x.reshape((1,)+x.shape)\n",
    "        #x=x/255.0\n",
    "\n",
    "        for i in datagen.flow(x):\n",
    "            if counter>=1:\n",
    "                break\n",
    "            #i=i/255.0\n",
    "            #i = cv2.resize(i,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "            aug_pneumonia.append(i)\n",
    "            counter+=1    \n",
    "\n",
    "    for ele in normal_files:\n",
    "        #ele=ele/255.0\n",
    "        x = Image.open(ele).convert('L')\n",
    "        x = asarray(x)\n",
    "        \n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        x = clahe.apply(x)\n",
    "        pic = cv2.resize(x,(256,256),interpolation = cv2.INTER_CUBIC)\n",
    "        pic=pic/255.0\n",
    "        aug_normal.append(pic)\n",
    "    for ele in covid_files:\n",
    "        #ele=ele/255.0\n",
    "        x = Image.open(ele).convert('L')\n",
    "        x = asarray(x)\n",
    "        \n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        x = clahe.apply(x)\n",
    "        pic = cv2.resize(x,(256,256),interpolation = cv2.INTER_CUBIC)\n",
    "        pic=pic/255.0\n",
    "        aug_covid.append(pic)\n",
    "    for ele in pneumonia_files:\n",
    "        #ele=ele/255.0\n",
    "        x = Image.open(ele).convert('L')\n",
    "        x = asarray(x)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        x = clahe.apply(x)\n",
    "        pic = cv2.resize(x,(256,256),interpolation = cv2.INTER_CUBIC)\n",
    "        pic=pic/255.0\n",
    "        aug_pneumonia.append(pic)\n",
    "    for i in range(len(aug_normal)):\n",
    "        aug_normal[i]=aug_normal[i].reshape((256,256))\n",
    "    \n",
    "    for i in range(len(aug_covid)):\n",
    "        aug_covid[i]=aug_covid[i].reshape((256,256))\n",
    "    for i in range(len(aug_pneumonia)):\n",
    "        aug_pneumonia[i]=aug_pneumonia[i].reshape((256,256))\n",
    "    \n",
    "    print(\"Normal files after augmentation:\",len(aug_normal))\n",
    "    print(\"Covid files after augmentation:\", len(aug_covid))\n",
    "    print(\"Pneumonia files after augmentation:\",len(aug_pneumonia))\n",
    "    return aug_normal,aug_covid,aug_pneumonia\n",
    "\n",
    "def making_full_data(aug_normal,aug_covid,aug_pneumonia):\n",
    "    aug_normal=shuffle(aug_normal, random_state=0)\n",
    "    aug_covid=shuffle(aug_covid,random_state=0)\n",
    "    aug_pneumonia=shuffle(aug_pneumonia,random_state=0)\n",
    "    \n",
    "    aug_normal_labels=[]\n",
    "    for i in range(len(aug_normal)):\n",
    "        aug_normal_labels.append(0)\n",
    "    print(np.shape(aug_normal),np.shape(aug_normal_labels))\n",
    "    aug_covid_labels=[]\n",
    "    for i in range(len(aug_covid)):\n",
    "        aug_covid_labels.append(1)\n",
    "    print(np.shape(aug_covid),np.shape(aug_covid_labels))\n",
    "    aug_pneumonia_labels=[]\n",
    "    for i in range(len(aug_pneumonia)):\n",
    "        aug_pneumonia_labels.append(2)\n",
    "    print(np.shape(aug_pneumonia),np.shape(aug_pneumonia_labels))  \n",
    "\n",
    "    full_data=[]\n",
    "    full_label=[]\n",
    "    for i in range(len(aug_normal)):\n",
    "        full_data.append(aug_normal[i])\n",
    "        full_label.append(aug_normal_labels[i])\n",
    "    for i in range(len(aug_covid)):\n",
    "        full_data.append(aug_covid[i])\n",
    "        full_label.append(aug_covid_labels[i])\n",
    "    for i in range(len(aug_pneumonia)):\n",
    "        full_data.append(aug_pneumonia[i])\n",
    "        full_label.append(aug_pneumonia_labels[i])\n",
    "        \n",
    "    full_data=np.array(full_data)\n",
    "    full_label=np.array(full_label)\n",
    "    \n",
    "    full_data=shuffle(full_data,random_state=0)\n",
    "    full_label=shuffle(full_label,random_state=0)\n",
    "    \n",
    "    return full_data,full_label\n",
    "\"\"\"Inception 2D_CNN Models in Tensorflow-Keras.\n",
    "References -\n",
    "Inception_v1 (GoogLeNet): https://arxiv.org/abs/1409.4842 [Going Deeper with Convolutions]\n",
    "Inception_v2: http://arxiv.org/abs/1512.00567 [Rethinking the Inception Architecture for Computer Vision]\n",
    "Inception_v3: http://arxiv.org/abs/1512.00567 [Rethinking the Inception Architecture for Computer Vision]\n",
    "Inception_v4: https://arxiv.org/abs/1602.07261 [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning]\n",
    "\"\"\"\n",
    "def making_training_and_testing_data(full_data,full_label):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(full_data, full_label, test_size=0.20, random_state=42)\n",
    "    \n",
    "    train_label=[]\n",
    "    for i in range(len(y_train)):\n",
    "        if y_train[i]==0:\n",
    "            train_label.append([0,1,0])\n",
    "        elif y_train[i]==1:\n",
    "            train_label.append([1,0,0])\n",
    "        elif y_train[i]==2:\n",
    "\n",
    "            train_label.append([0,0,1])\n",
    "\n",
    "    test_label=[]\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i]==0:\n",
    "            test_label.append([0,1,0])\n",
    "        elif y_test[i]==1:\n",
    "            test_label.append([1,0,0])\n",
    "        elif y_test[i]==2:\n",
    "\n",
    "            test_label.append([0,0,1])\n",
    "    y_train=np.array(train_label)\n",
    "    y_test=np.array(test_label)\n",
    "    \n",
    "    return X_train,X_test,y_train,y_test\n",
    "    \n",
    "def my_plots(history,my_model):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    my_path=\"training and validation accuracy curve of \"+my_model+\".png\"\n",
    "    plt.savefig(my_path)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "    #plt.ylim([-3, 3])\n",
    "    plt.yticks(np.arange(0, 1.1, 0.25))\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    my_path=\"training and validation loss curve of \"+my_model+\".png\"\n",
    "    plt.savefig(my_path)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    normal_dir = \"/home/pranab_2021cs25/Shubham_Amity/ViT_datasets/ViT_dataset/Normal/\" #give your normal cases data path here\n",
    "    #vit_datasets/Dataset_ViT/ViT_dataset/Covid-19\n",
    "    dir1 = os.path.join(normal_dir,\"*.png\")\n",
    "    dir2 = os.path.join(normal_dir,\"*.jpeg\")\n",
    "    dir = os.path.join(normal_dir,\"*.jpg\")\n",
    "    normal_files = glob.glob(dir)\n",
    "    normal_1 = glob.glob(dir1)\n",
    "    normal_2 = glob.glob(dir2)\n",
    "    normal_files.extend(normal_1)\n",
    "    normal_files.extend(normal_2)\n",
    "\n",
    "    normal_dir = \"/home/pranab_2021cs25/Shubham_Amity/ViT_datasets/ViT_dataset/Covid-19/\"  #give your covid 19 cases data path here\n",
    "    dir1 = os.path.join(normal_dir,\"*.png\")\n",
    "    dir = os.path.join(normal_dir,\"*.jpg\")\n",
    "    dir2 = os.path.join(normal_dir,\"*.jpeg\")\n",
    "    covid_files = glob.glob(dir)\n",
    "    covid_files2 = glob.glob(dir2)\n",
    "    covid_files1 = glob.glob(dir1)\n",
    "    covid_files.extend(covid_files2)\n",
    "    covid_files.extend(covid_files1)\n",
    "\n",
    "    normal_dir = \"/home/pranab_2021cs25/Shubham_Amity/ViT_datasets/ViT_dataset/Pneumonia/\" #give your pneumonia cases data path here\n",
    "    dir1 = os.path.join(normal_dir,\"*.png\")\n",
    "    dir2 = os.path.join(normal_dir,\"*.jpeg\")\n",
    "    dir = os.path.join(normal_dir,\"*.jpg\")\n",
    "    pneumonia_files = glob.glob(dir)\n",
    "    pneumonia_1 = glob.glob(dir1)\n",
    "    pneumonia_2 = glob.glob(dir2)\n",
    "    pneumonia_files.extend(pneumonia_1)\n",
    "    pneumonia_files.extend(pneumonia_2)\n",
    "\n",
    "    normal_files.sort()\n",
    "    covid_files.sort()\n",
    "    pneumonia_files.sort()\n",
    "    \n",
    "    aug_normal,aug_covid,aug_pneumonia=data_augmentation(normal_files,covid_files,pneumonia_files)\n",
    "    \n",
    "    full_data,full_label=making_full_data(aug_normal,aug_covid,aug_pneumonia)  #getting my full data\n",
    "    \n",
    "    X_train,X_test,y_train,y_test= making_training_and_testing_data(full_data,full_label) #dividing full_data into train and test data\n",
    "    print(\"Multiplying 3 times\")\n",
    "    train_data=np.stack((X_train,)*3,axis=-1)\n",
    "    test_data=np.stack((X_test,)*3,axis=-1)\n",
    "    print(\"Multiplying done\")\n",
    "    learning_rate = 0.0001\n",
    "    weight_decay = 0.0001\n",
    "    batch_size = 16\n",
    "    num_epochs = 100\n",
    "    print(\"now model initialization part\")\n",
    "    if model_name==1:  #IT WILL RUN MOBILE VIT MODEL\n",
    "        model = MobileViT_S((256, 256, 3), 3)\n",
    "#         optimizer = tfa.optimizers.AdamW(\n",
    "#         learning_rate=learning_rate, weight_decay=weight_decay\n",
    "#         )\n",
    "#         print(\"model compiling\")\n",
    "#         model.compile(\n",
    "#             optimizer=optimizer,\n",
    "#             loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "#             metrics=[\n",
    "#                 keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "#                 keras.metrics.SparseTopKCategoricalAccuracy(2, name=\"top-5-accuracy\"),\n",
    "#             ],\n",
    "#         )\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss=tf.keras.losses.CategoricalCrossentropy(from_logits = False), metrics=['accuracy'])\n",
    "\n",
    "        print(\"compiling done\")\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', \n",
    "            patience=45, \n",
    "\n",
    "            min_delta=0.001, \n",
    "            mode='min')\n",
    "        print(\"model starting fitting\")\n",
    "        history=model.fit(train_data,y_train,epochs=100,validation_data=(test_data, y_test),callbacks=[early_stopping],batch_size=16)\n",
    "        #history=model.fit(train_data,y_train,epochs=100,validation_data=(test_data, y_test),callbacks=[early_stopping],batch_size=16)\n",
    "        np.save('my_history_mobvit_with_clahe_fl.npy',history.history)\n",
    "        my_plots(history,\"MOBILE_VIT_with_clahe_fl\")\n",
    "        filename = 'MOBILE_VIT_model_with_clahe_fl.sav'\n",
    "        pickle.dump(model, open(filename, 'wb'))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
